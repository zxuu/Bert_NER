{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text   \n0  SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRI...  \\\n1                                        Nadim Ladki   \n2           AL-AIN , United Arab Emirates 1996-12-06   \n3  Japan began the defence of their Asian Cup tit...   \n4  But China saw their luck desert them in the se...   \n\n                                              labels  \n0                    O O B-LOC O O O O B-PER O O O O  \n1                                        B-PER I-PER  \n2                        B-LOC O B-LOC I-LOC I-LOC O  \n3  B-LOC O O O O O B-MISC I-MISC O O O O O O O B-...  \n4  O B-LOC O O O O O O O O O O O O O O O O O O O ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRI...</td>\n      <td>O O B-LOC O O O O B-PER O O O O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Nadim Ladki</td>\n      <td>B-PER I-PER</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AL-AIN , United Arab Emirates 1996-12-06</td>\n      <td>B-LOC O B-LOC I-LOC I-LOC O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Japan began the defence of their Asian Cup tit...</td>\n      <td>B-LOC O O O O O B-MISC I-MISC O O O O O O O B-...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>But China saw their luck desert them in the se...</td>\n      <td>O B-LOC O O O O O O O O O O O O O O O O O O O ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/ner/ner.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-ORG', 'B-LOC', 'B-MISC', 'I-PER', 'I-LOC', 'I-MISC', 'O', 'B-PER', 'B-ORG'}\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"\\n{'I-ORG',\\n 'O',\\n'B-MISC',\\n'B-ORG',\\n'I-LOC',\\n'I-PER',\\n'I-MISC',\\n'B-PER',\\n'B-LOC'}\\n\""
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 根据空格拆分标签，并将它们转换为列表\n",
    "labels = [i.split() for i in df['labels'].values.tolist()]\n",
    "# 检查数据集中有多少标签\n",
    "unique_labels = set()\n",
    "for lb in labels:\n",
    "  [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
    "print(unique_labels)\n",
    "'''\n",
    "{'I-ORG',\n",
    " 'O',\n",
    "'B-MISC',\n",
    "'B-ORG',\n",
    "'I-LOC',\n",
    "'I-PER',\n",
    "'I-MISC',\n",
    "'B-PER',\n",
    "'B-LOC'}\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-LOC': 0, 'B-MISC': 1, 'B-ORG': 2, 'B-PER': 3, 'I-LOC': 4, 'I-MISC': 5, 'I-ORG': 6, 'I-PER': 7, 'O': 8}\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"\\n{'B-LOC': 0,\\n 'B-MISC': 1,\\n'B-ORG': 2,\\n'B-PER': 3,\\n'I-LOC': 4,\\n'I-MISC': 5,\\n'I-ORG': 6,\\n'I-PER': 7,\\n'O': 8}\\n\""
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
    "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
    "print(labels_to_ids)\n",
    "'''\n",
    "{'B-LOC': 0,\n",
    " 'B-MISC': 1,\n",
    "'B-ORG': 2,\n",
    "'B-PER': 3,\n",
    "'I-LOC': 4,\n",
    "'I-MISC': 5,\n",
    "'I-ORG': 6,\n",
    "'I-PER': 7,\n",
    "'O': 8}\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two goals in the last six minutes gave holders Japan an uninspiring 2-1 Asian Cup victory over Syria on Friday .\n"
     ]
    }
   ],
   "source": [
    "text = df['text'].values.tolist()\n",
    "example = text[36]\n",
    "print(example)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "# text_tokenized = tokenizer(example, padding='max_length',\n",
    "#                            max_length=512, truncation=True,\n",
    "#                            return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1960,  2513,  1107,  1103,  1314,  1565,  1904,  1522, 14322,\n",
      "          1999,  1126,  8362,  4935,  8508,  3384,   123,   118,   122,  3141,\n",
      "          1635,  2681,  1166,  7303,  1113,  5286,   119,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "print(text_tokenized)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Two goals in the last six minutes gave holders Japan an uninspiring 2 - 1 Asian Cup victory over Syria on Friday. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(text_tokenized.input_ids[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Two', 'goals', 'in', 'the', 'last', 'six', 'minutes', 'gave', 'holders', 'Japan', 'an', 'un', '##ins', '##pi', '##ring', '2', '-', '1', 'Asian', 'Cup', 'victory', 'over', 'Syria', 'on', 'Friday', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "word_ids = text_tokenized.word_ids()\n",
    "print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0])) # 512\n",
    "print(word_ids)    # 512"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "通过这些 word_ids，并使用以下两种方法来调整标签的长度：\n",
    "\n",
    "1\\只为每个拆分token的第一个子词提供一个标签。子词的延续将简单地用\"-100\"作为标签。所有没有word_ids 的token也将标为 \"-100\"。\n",
    "\n",
    "2\\在属于同一 token 的所有子词中提供相同的标签。所有没有word_ids的token都将标为 \"-100\"。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def align_label_example(tokenized_input, labels):\n",
    "        word_ids = tokenized_input.word_ids()\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                try:\n",
    "                  label_ids.append(labels_to_ids[labels[word_idx]])\n",
    "                except:\n",
    "                  label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        return label_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'B-LOC', 'O', 'O', 'O']\n",
      "[-100, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, 8, 8, 8, 8, 1, 5, 8, 8, 0, 8, 8, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "['[CLS]', 'Two', 'goals', 'in', 'the', 'last', 'six', 'minutes', 'gave', 'holders', 'Japan', 'an', 'un', '##ins', '##pi', '##ring', '2', '-', '1', 'Asian', 'Cup', 'victory', 'over', 'Syria', 'on', 'Friday', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "#设置label_all_tokens=True\n",
    "label = labels[36]\n",
    "label_all_tokens = True\n",
    "print(label)\n",
    "new_label = align_label_example(text_tokenized, label)\n",
    "print(new_label)\n",
    "print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two goals in the last six minutes gave holders Japan an uninspiring 2-1 Asian Cup victory over Syria on Friday .\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'B-LOC', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(example)\n",
    "print(label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-100 ,  [CLS] , \n",
      "8 ,  Two , \n",
      "8 ,  goals , \n",
      "8 ,  in , \n",
      "8 ,  the , \n",
      "8 ,  last , \n",
      "8 ,  six , \n",
      "8 ,  minutes , \n",
      "8 ,  gave , \n",
      "8 ,  holders , \n",
      "0 ,  Japan , \n",
      "8 ,  an , \n",
      "8 ,  un , \n",
      "8 ,  ##ins , \n",
      "8 ,  ##pi , \n",
      "8 ,  ##ring , \n",
      "8 ,  2 , \n",
      "1 ,  - , \n",
      "5 ,  1 , \n",
      "8 ,  Asian , \n",
      "8 ,  Cup , \n",
      "0 ,  victory , \n",
      "8 ,  over , \n",
      "8 ,  Syria , \n",
      "8 ,  on , \n",
      "-100 ,  Friday , \n",
      "-100 ,  . , \n",
      "-100 ,  [SEP] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n",
      "-100 ,  [PAD] , \n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "t = tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0])\n",
    "for i in range(len(t)):\n",
    "    print(new_label[i],', ',t[i],', ')\n",
    "    s = s + 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 8, 8, -100, -100, -100, 8, 1, 5, 8, 8, 0, 8, 8, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "['[CLS]', 'Two', 'goals', 'in', 'the', 'last', 'six', 'minutes', 'gave', 'holders', 'Japan', 'an', 'un', '##ins', '##pi', '##ring', '2', '-', '1', 'Asian', 'Cup', 'victory', 'over', 'Syria', 'on', 'Friday', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# 设置label_all_tokens=False\n",
    "label_all_tokens = False\n",
    "new_label = align_label_example(text_tokenized, label)\n",
    "print(new_label)\n",
    "print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-100 ,  [CLS]\n",
      "8 ,  Two\n",
      "8 ,  goals\n",
      "8 ,  in\n",
      "8 ,  the\n",
      "8 ,  last\n",
      "8 ,  six\n",
      "8 ,  minutes\n",
      "8 ,  gave\n",
      "8 ,  holders\n",
      "0 ,  Japan\n",
      "8 ,  an\n",
      "8 ,  un\n",
      "8 ,  ##ins\n",
      "8 ,  ##pi\n",
      "8 ,  ##ring\n",
      "8 ,  2\n",
      "1 ,  -\n",
      "5 ,  1\n",
      "8 ,  Asian\n",
      "8 ,  Cup\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "t = tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0])\n",
    "for i in range(len(t)):\n",
    "    print(new_label[i],', ',t[i])\n",
    "    s = s + 1\n",
    "    if s>20:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def align_label(texts, labels):\n",
    "    # 首先tokenizer输入文本\n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
    "  # 获取word_ids\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    # 采用上述的第一中方法来调整标签，使得标签与输入数据对其。\n",
    "    for word_idx in word_ids:\n",
    "        # 如果token不在word_ids内，则用 “-100” 填充\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        # 如果token在word_ids内，且word_idx不为None，则从labels_to_ids获取label id\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]])\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        # 如果token在word_ids内，且word_idx为None\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids\n",
    "# 构建自己的数据集类\n",
    "class DataSequence(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        # 根据空格拆分labels\n",
    "        lb = [i.split() for i in df['labels'].values.tolist()]\n",
    "        # tokenizer 向量化文本\n",
    "        txt = df['text'].values.tolist()\n",
    "        self.texts = [tokenizer(str(i),\n",
    "                               padding='max_length', max_length = 512,\n",
    "                                truncation=True, return_tensors=\"pt\") for i in txt]\n",
    "        # 对齐标签\n",
    "        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_data(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return torch.LongTensor(self.labels[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_data = self.get_batch_data(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "        return batch_data, batch_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = df[0:500]\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),\n",
    "                            [int(.8 * len(df)), int(.9 * len(df))])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# bert model\n",
    "from transformers import BertForTokenClassification\n",
    "class BertModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.bert = BertForTokenClassification.from_pretrained(\n",
    "                       'bert-base-cased',\n",
    "                                     num_labels=len(unique_labels))\n",
    "\n",
    "    def forward(self, input_id, mask, label):\n",
    "        output = self.bert(input_ids=input_id, attention_mask=mask,\n",
    "                           labels=label, return_dict=False)\n",
    "        return output\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "训练"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.sgd import SGD\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_loop(model, df_train, df_val):\n",
    "    # 定义训练和验证集数据\n",
    "    train_dataset = DataSequence(df_train)\n",
    "    val_dataset = DataSequence(df_val)\n",
    "    # 批量获取训练和验证集数据\n",
    "    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=1, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=1)\n",
    "    # 判断是否使用GPU，如果有，尽量使用，可以加快训练速度\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    # 定义优化器\n",
    "    optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    # 开始训练循环\n",
    "    best_acc = 0\n",
    "    best_loss = 1000\n",
    "    for epoch_num in range(EPOCHS):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        # 训练模型\n",
    "        model.train()\n",
    "        # 按批量循环训练模型\n",
    "        for train_data, train_label in tqdm(train_dataloader):\n",
    "      # 从train_data中获取mask和input_id\n",
    "            train_label = train_label[0].to(device)\n",
    "            mask = train_data['attention_mask'][0].to(device)\n",
    "            input_id = train_data['input_ids'][0].to(device)\n",
    "            # 梯度清零！！\n",
    "            optimizer.zero_grad()\n",
    "            # 输入模型训练结果：损失及分类概率\n",
    "            loss, logits = model(input_id, mask, train_label)\n",
    "            # 过滤掉特殊token及padding的token\n",
    "            logits_clean = logits[0][train_label != -100]\n",
    "            label_clean = train_label[train_label != -100]\n",
    "            # 获取最大概率值\n",
    "            predictions = logits_clean.argmax(dim=1)\n",
    "      # 计算准确率\n",
    "            acc = (predictions == label_clean).float().mean()\n",
    "            total_acc_train += acc\n",
    "            total_loss_train += loss.item()\n",
    "      # 反向传递\n",
    "            loss.backward()\n",
    "            # 参数更新\n",
    "            optimizer.step()\n",
    "        # 模型评估\n",
    "        model.eval()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "        for val_data, val_label in val_dataloader:\n",
    "      # 批量获取验证数据\n",
    "            val_label = val_label[0].to(device)\n",
    "            mask = val_data['attention_mask'][0].to(device)\n",
    "            input_id = val_data['input_ids'][0].to(device)\n",
    "      # 输出模型预测结果\n",
    "            loss, logits = model(input_id, mask, val_label)\n",
    "      # 清楚无效token对应的结果\n",
    "            logits_clean = logits[0][val_label != -100]\n",
    "            label_clean = val_label[val_label != -100]\n",
    "            # 获取概率值最大的预测\n",
    "            predictions = logits_clean.argmax(dim=1)\n",
    "            # 计算精度\n",
    "            acc = (predictions == label_clean).float().mean()\n",
    "            total_acc_val += acc\n",
    "            total_loss_val += loss.item()\n",
    "\n",
    "        val_accuracy = total_acc_val / len(df_val)\n",
    "        val_loss = total_loss_val / len(df_val)\n",
    "\n",
    "        print(\n",
    "            f'''Epochs: {epoch_num + 1} |\n",
    "                Loss: {total_loss_train / len(df_train): .3f} |\n",
    "                Accuracy: {total_acc_train / len(df_train): .3f} |\n",
    "                Val_Loss: {total_loss_val / len(df_val): .3f} |\n",
    "                Accuracy: {total_acc_val / len(df_val): .3f}''')\n",
    "\n",
    "LEARNING_RATE = 1e-2\n",
    "EPOCHS = 5\n",
    "model = BertModel()\n",
    "train_loop(model, df_train, df_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "测试"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate(model, df_test):\n",
    "    # 定义测试数据\n",
    "    test_dataset = DataSequence(df_test)\n",
    "    # 批量获取测试数据\n",
    "    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n",
    "   # 使用GPU\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    total_acc_test = 0.0\n",
    "    for test_data, test_label in test_dataloader:\n",
    "        test_label = test_label[0].to(device)\n",
    "        mask = test_data['attention_mask'][0].to(device)\n",
    "        input_id = test_data['input_ids'][0].to(device)\n",
    "\n",
    "        loss, logits = model(input_id, mask, test_label.long())\n",
    "        logits_clean = logits[0][test_label != -100]\n",
    "        label_clean = test_label[test_label != -100]\n",
    "        predictions = logits_clean.argmax(dim=1)\n",
    "        acc = (predictions == label_clean).float().mean()\n",
    "        total_acc_test += acc\n",
    "    val_accuracy = total_acc_test / len(df_test)\n",
    "    print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n",
    "\n",
    "evaluate(model, df_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "预测"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def align_word_ids(texts):\n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(1)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(1 if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "    return label_ids\n",
    "\n",
    "def evaluate_one_text(model, sentence):\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "    mask = text['attention_mask'][0].unsqueeze(0).to(device)\n",
    "    input_id = text['input_ids'][0].unsqueeze(0).to(device)\n",
    "    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n",
    "\n",
    "    logits = model(input_id, mask, None)\n",
    "    logits_clean = logits[0][label_ids != -100]\n",
    "\n",
    "    predictions = logits_clean.argmax(dim=1).tolist()\n",
    "    prediction_label = [ids_to_labels[i] for i in predictions]\n",
    "    print(sentence)\n",
    "    print(prediction_label)\n",
    "\n",
    "evaluate_one_text(model, 'Bill Gates is the founder of Microsoft')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-12f910e3",
   "language": "python",
   "display_name": "PyCharm (huggingface2)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}